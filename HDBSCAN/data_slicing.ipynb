{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9305c1",
   "metadata": {},
   "source": [
    "# Data slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "7caee624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = np.load(\"cluster_labels_min_df_5.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e5bd797e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise: 108746 points\n",
      "Cluster 0: 69 points\n",
      "Cluster 1: 97 points\n",
      "Cluster 2: 89 points\n",
      "Cluster 3: 159 points\n",
      "Cluster 4: 78 points\n",
      "Cluster 5: 181 points\n",
      "Cluster 6: 250 points\n",
      "Cluster 7: 493 points\n",
      "Cluster 8: 1346 points\n",
      "Cluster 9: 470 points\n",
      "Cluster 10: 206 points\n",
      "Cluster 11: 144 points\n",
      "Cluster 12: 273 points\n",
      "Cluster 13: 510 points\n",
      "Cluster 14: 195 points\n",
      "Cluster 15: 69 points\n",
      "Cluster 16: 187 points\n",
      "Cluster 17: 355 points\n",
      "Cluster 18: 205 points\n",
      "Cluster 19: 227 points\n",
      "Cluster 20: 69 points\n",
      "Cluster 21: 136 points\n",
      "Cluster 22: 233 points\n",
      "Cluster 23: 165 points\n",
      "Cluster 24: 162 points\n",
      "Cluster 25: 663 points\n",
      "Cluster 26: 106 points\n",
      "Cluster 27: 928 points\n",
      "Cluster 28: 86 points\n",
      "Cluster 29: 165 points\n",
      "Cluster 30: 87 points\n",
      "Cluster 31: 543 points\n",
      "Cluster 32: 652 points\n",
      "Cluster 33: 129 points\n",
      "Cluster 34: 703 points\n",
      "Cluster 35: 93 points\n",
      "Cluster 36: 39189 points\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_counts = Counter(labels)\n",
    "for label, count in sorted(label_counts.items()):\n",
    "    if label == -1:\n",
    "        print(f\"Noise: {count} points\")\n",
    "    else:\n",
    "        print(f\"Cluster {label}: {count} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fa6e1485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting sequences...: 100%|██████████| 400000/400000 [00:04<00:00, 86466.83it/s] \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('../dataset/movie_new2.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "def extract_sequence(line):\n",
    "    pairs = re.findall(r'\\((\\d+),\\s*(\\d+)\\)', line)\n",
    "    sequence = [int(pairs[0][0])] + [int(p[1]) for p in pairs]\n",
    "    return sequence\n",
    "sequences = [extract_sequence(line) for line in tqdm(lines, desc=\"Extracting sequences...\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a2cf2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ids = sorted(set(movie_id for seq in sequences for movie_id in seq))\n",
    "movie_id_to_cluster = {movie_id: cluster_label for movie_id, cluster_label in zip(movie_ids, labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "73d224e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "cluster_to_movies = defaultdict(list)\n",
    "for m, c in movie_id_to_cluster.items():\n",
    "    if c != -1:  # -1 通常是 noise\n",
    "        cluster_to_movies[c].append(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "33cc0e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_to_user_sequences = defaultdict(list)\n",
    "cluster_to_seen_sequences = defaultdict(set)  # 新增：追蹤每個 cluster 已經加入過哪些序列\n",
    "\n",
    "for seq in sequences: \n",
    "\n",
    "    movie_list = set(seq)\n",
    "\n",
    "    if(len(movie_list) < 15):\n",
    "        continue\n",
    "\n",
    "    # 找出此 user 涉及到的所有 cluster\n",
    "    clusters_hit = set()\n",
    "    for movie_id in movie_list:\n",
    "        if movie_id in movie_id_to_cluster:\n",
    "            clusters_hit.add(movie_id_to_cluster[movie_id])\n",
    "\n",
    "    # 把該序列加到對應 cluster 的新資料集中\n",
    "    for c in clusters_hit:\n",
    "        seq_tuple = tuple(seq)  # list 無法 hash，轉成 tuple\n",
    "        if seq_tuple not in cluster_to_seen_sequences[c]:\n",
    "            cluster_to_user_sequences[c].append(seq)\n",
    "            cluster_to_seen_sequences[c].add(seq_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2ab94591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster_to_user_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cc15d0",
   "metadata": {},
   "source": [
    "## 合併相似 cluster，讓所有 cluster 大小都大於 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8f784340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import load_npz\n",
    "X_tfidf = load_npz(\"../data_preprocessing/X_tfidf_sparse_min_df_5.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "94071103",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 建立 cluster_centroids：每個 cluster 的 TF-IDF 中心向量\n",
    "cluster_centroids = {}\n",
    "for c in cluster_to_user_sequences:\n",
    "    indices = [i for i, seq in enumerate(sequences) \n",
    "                if any(mid in movie_id_to_cluster and movie_id_to_cluster[mid] == c for mid in seq)]\n",
    "    if indices:\n",
    "        centroid = np.asarray(X_tfidf[indices].mean(axis=0))\n",
    "        cluster_centroids[c] = (centroid)\n",
    "# 合併小群\n",
    "threshold = 10000\n",
    "updated = True\n",
    "\n",
    "while updated:\n",
    "    updated = False\n",
    "\n",
    "    # 找出所有小群（每次都重算，因為數量在變）\n",
    "    small_clusters = [c for c in cluster_to_user_sequences if len(cluster_to_user_sequences[c]) < threshold]\n",
    "    if not small_clusters:\n",
    "        break\n",
    "\n",
    "    for cid in small_clusters:\n",
    "        if cid not in cluster_centroids:\n",
    "            continue\n",
    "\n",
    "        profile = np.asarray(cluster_centroids[cid])\n",
    "        # 候選包含所有其他群（不限制大小）\n",
    "        candidates = [other for other in cluster_centroids if other != cid]\n",
    "\n",
    "        if not candidates:\n",
    "            continue\n",
    "\n",
    "        similarities = [\n",
    "            cosine_similarity(profile, np.asarray(cluster_centroids[other]))[0, 0]\n",
    "            for other in candidates\n",
    "        ]\n",
    "        best_idx = np.argmax(similarities)\n",
    "        merge_to = candidates[best_idx]\n",
    "\n",
    "        # 合併 sequence\n",
    "        cluster_to_user_sequences[merge_to].extend(cluster_to_user_sequences[cid])\n",
    "\n",
    "        # 更新中心向量\n",
    "        indices_merge = [\n",
    "            i for i, seq in enumerate(sequences)\n",
    "            if any(mid in movie_id_to_cluster and movie_id_to_cluster[mid] == merge_to for mid in seq)\n",
    "        ]\n",
    "        cluster_centroids[merge_to] = np.asarray(X_tfidf[indices_merge].mean(axis=0))\n",
    "\n",
    "        # 更新映射（movie_id_to_cluster）\n",
    "        for mid, cl in movie_id_to_cluster.items():\n",
    "            if cl == cid:\n",
    "                movie_id_to_cluster[mid] = merge_to\n",
    "\n",
    "        # 移除舊 cluster\n",
    "        del cluster_to_user_sequences[cid]\n",
    "        del cluster_centroids[cid]\n",
    "\n",
    "        updated = True\n",
    "        break  # 每次只做一個，重新檢查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c88adda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(small_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "01503ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "print(len(cluster_to_user_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58aceed",
   "metadata": {},
   "source": [
    "## Store the slicing data to each cluster file (cluster_{id}.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4498f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "cluster_folder = \"../dataset/cluster_movie\"\n",
    "\n",
    "# 若資料夾存在，先刪除其內容；若不存在則建立\n",
    "if os.path.exists(cluster_folder):\n",
    "    # 刪除裡面所有檔案\n",
    "    for filename in os.listdir(cluster_folder):\n",
    "        file_path = os.path.join(cluster_folder, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "else:\n",
    "    os.makedirs(cluster_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b9cef38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id, sequences in cluster_to_user_sequences.items():\n",
    "    filename = f\"../dataset/cluster_movie/cluster_{cluster_id}.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for seq in sequences:\n",
    "            pairs = [f\"({seq[i]}, {seq[i+1]})\" for i in range(len(seq) - 1)]\n",
    "            line = \"#\".join(pairs)\n",
    "            f.write(line + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
